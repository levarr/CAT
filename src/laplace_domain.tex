% !TEX root = ./main.tex

\part{Analisi nel dominio di Laplace}

\chapter{Laplace $\mathcal{L}\sparen{\cdot}$}
Come già visto in altri ambiti con Steinmetz e Fourier, anche in questo corso può rivelarsi utile spostare un problema dal dominio temporale ad un altro, risolverlo lì, e riportare la soluzione di nuovo nel tempo.
\resource{0.5}{laplace}{Risoluzione nel dominio di Laplace}
I concetti che vedremo faranno forte uso dei numeri complessi, che però \textbf{non} verranno ripresi qui perché sono sempre le solite cose (rappresentazione cartesiana, polare, esponenziale, come passare dall'una all'altra, proprietà varie). Rimando alla dispensa di TLC-1 dove queste cose sono trattate di più.
\section{Trasformata}
\begin{defin}{}{}
Sia $f: \R \rightarrow \C$ (in questo corso tipicamente $f: \R \rightarrow \R$); sia $s = \sigma + j\omega \in \C$. Definiamo la \textbf{trasformata di Laplace} (se esiste) come:
\begin{equation}
	\lt{f(t)} = F(s) = \ltint{f(t)}{-st}{t}
\end{equation} 
Ci muoviamo, quindi, dal dominio del tempo, al \textbf{dominio di Laplace}: $f(t) \xrightarrow{\Lap} F(s)$. Notiamo gli estremi di integrazione: Laplace non è definita, come Fourier, su tutto l'asse reale, ma solo sul \textbf{semiasse positivo} $[0^-, \pinf]$. Lo $0^-$ è incluso perché bisogna tenere conto, in fase di trasformazione, di eventuali \textbf{impulsi in $t = 0$ della $f$}. La funzione integranda è complesso, per cui l'integrale stesso (e dunque $F(s)$) è complesso. \bb Dato il dominio di definizione, si può dire che l'operazione di trasformazione \textbf{non è biunivoca}, i.e. non è vero che ad ogni $f$ corrisponde una e una sola $F$. Potrei prendere delle funzioni del tutto diverse tra loro nell'intervallo $\R^-$, ma poi identiche in $\R^+$: queste avranno la stessa trasformata. Per ottenere biunivocità, considereremo  sempre \textbf{funzioni definite nel semiasse positivo di $\R$, e nulle altrove} (dette \textbf{funzioni causali)}:
\begin{equation}
	f(t) = 0 \quad \forall t < 0^-
\end{equation}
Supponendo di lavorare con sistemi \textit{tempo invarianti}, questa restrizione non comporta perdità di generalità: posso traslare una qualsiasi funzione TI di un certo valore temporale $t_0$ per riportarla completamente nel dominio $[0^-, \pinf]$, e si comporterà identicamente a come si comporterebbe se non fosse traslata. 
\end{defin}

\begin{prop}
Si può far vedere che:
\begin{equation}
	\exists \bar \sigma \ : \ \forall s = \sigma + j \omega \ \textrm{avente} \ \sigma > \bar \sigma \ \longrightarrow \ F(s) = \int \cdots \dd{t} \ \textrm{converge}.
\end{equation}
Esiste cioè un'ascissa $\bar \sigma$ sul piano complesso, detta \textbf{di convergenza}, a destra della quale si trovano tutti i complessi che fanno \textbf{convergere l'integrale} della trasformazione di Laplace. Dunque, \textbf{la trasformata esiste a destra di quest'ascissa} (i.e. nel semipiano $\Re{s} > \bar \sigma$), (ma sarà possibile estendere l'esistenza anche a sinistra di $\bar \sigma$, i.e. per $\Re{s} \leq \bar \sigma$).
\end{prop}

\subsection{Trasformata come rapporto di polinomi}
Vedremo nel corso $f(t)$ aventi sempre come trasformata un \textbf{rapporto di polinomi} (a coefficienti reali, se $f$ è reale):
\begin{equation}
\label{lt_rap}
	F(s) = \frac{N(s)}{D(s)}
\end{equation} 
Le radici del numeratore ($N(s) = 0$) sono dette \textbf{zeri}, quelle del denominatore, \textbf{poli}. Valgono i seguenti:
\begin{them}
(del valore iniziale). Se $f(t)$ è una funzione reale avente trasformata $F(s)$ esprimibile come in \eqref{lt_rap}, e \textbf{il grado di $D(s)$ è maggiore di quello di $N(s)$}, allora
\begin{equation}
\llimit{s}{\pinf}{sF(s)} = f(0).
\end{equation}
\end{them}

\begin{them} (del valore finale). Se alle ipotesi del teorema di cui sopra aggiungiamo anche che \textbf{il denominatore ha tutti i poli nulli o a parte reale negativa}, allora vale
\begin{equation}
\llimit{s}{0}{sF(s)} = \llimit{t}{\pinf}{f(t)}
\end{equation} 
\end{them}
Grazie a questi possiamo determinare i \textbf{valori asintotici }iniziali e finali di una funzione nel tempo \textbf{partendo dalla sua trasformata} di Laplace. Le dimostrazioni sono omesse.

\section{Antitrasformata}
\begin{defin}{}{}
Per tornare nel dominio del tempo ($F(s) \xrightarrow{\Lap^{-1}} f(t)$) usiamo la formula di \textbf{antitrasformazione}:
\begin{equation}
	f(t) = \lat{F(s)} = \latint{F(s)}{st}{s}, \quad \quad \sigma > \bar \sigma.
\end{equation}
Notiamo la restrizione per $\sigma$: abbiamo detto che $F(s)$ esiste a destra di $\bar \sigma$, quindi l'antitrasformazione (che integra $F(s)$ stesso) dovrà rispettare questo requisito. Questa formula non verrà mai utilizzata praticamente.
\end{defin}


\section{Proprietà}
\begin{itemize}
	\item \textbf{Linearità} \rarr \boxed{\lt{\alpha f(t) + \beta g(t)} = \alpha \lt{f(t)} + \beta \lt{g(t)}, \ \forall \alpha, \beta \in \R} \\ \\ si dimostra sfruttando la linearità dell'operatore integrale;
	\item \textbf{Traslazione temporale} \rarr \boxed{\lt{f(t-\tau)} = \lt{f(t)}e^{-s\tau}, \ \forall \tau > 0} : \\ \\ la trasformata di una funzione traslata sarà la trasformata della funzione non traslata, più un termine moltiplicativo che dipende dal quantitativo di traslazione.
	\begin{proof}
	Usiamo la definizione e consideriamo un cambio di variabile $y = t-\tau$, da cui $\dd{y} = \dd{t}$:
	\begin{align*}
		\lt{f(t-\tau)} = F(s) & = \ltint{f(t-\tau)}{-st}{t} = \int_{-\tau}^{\pinf} f(y)e^{-s(y+\tau)} \dd{y} = (\star)
	\end{align*}
	ma abbiamo detto di voler considerare funzioni nulle nel semiasse negativo, per cui possiamo cambiare l'estremo di integrazione:
	\begin{equation*}
		(\star) = \int_{0^-}^{\pinf} f(y)e^{-sy}\dd{y} e^{-s\tau} = \lt{f(t)}e^{-s\tau}.
	\end{equation*}
	\end{proof}
	\item \textbf{Traslazione nel dominio complesso} \rarr \boxed{\lt{e^{\alpha t}f(t)} = F(s-\alpha), \ \forall \alpha \in \C} : \\ \\ la traslazione nel dominio di Laplace  di un certo complesso $\alpha$ corrisponde ad un termine moltiplicativo nel dominio del tempo che dipende dalla translazione;
	\begin{proof}
		\begin{equation*}
		\lt{e^{\alpha t}f(t)}  = \ltint{e^{\alpha t}f(t)}{-st}{t} = \ltint{f(t)}{-(s-\alpha)t}{t} = F(s-\alpha) 		
		\end{equation*}
	\end{proof}
	\item \textbf{Derivazione (nel tempo)} \rarr \boxed{\lt{\dv{f(t)}{t}} = sF(s)-f(0^-)}
		\begin{proof}
	Usiamo la definizione di trasformata e applichiamo l'integrazione per parti:
	\begin{align*}
		\lt{\dv{f(t)}{t}} & = \ltint{\dv{f(t)}{t}}{-st}{t}	 = \eval{f(t)e^{-st}}_{0^-}^{\pinf} - \int_{0^-}^{\pinf} f(t)(-s)e^{-st} \dd{t} \\ & = 0 - f(0^-) + \int_{0^-}^{\pinf} sf(t)e^{-st} \dd{t} = - f(0^-) + s \int_{0^-}^{\pinf} f(t)e^{-st} \dd{t} \\ & = - f(0^-) + s \lt{f(t)} = sF(s) - f(0^-).
	\end{align*}
	\end{proof}
	\item \textbf{Derivazione (nel tempo) generica} \rarr \boxed{\lt{\dv[n]{f(t)}{t}} = s^{n} F(s) - \sum_{i=1}^{n} s^{n-i} \eval{\dv[i-1]{f(t)}{t}}_{t=0^-}} \footnote{
	Verifichiamo che è valida per la derivata prima:
	\begin{equation*}
		\lt{\dv{f(t)}{t}} = s^1 F(s) - \sum_{i=1}^1 \cdots = s F(s) - s^{0} \eval{f(t)}_{0^-} = sF(s)-f(0^-).
	\end{equation*}}
	\item \textbf{Integrazione (nel tempo)} \rarr \boxed{\lt{\int_0^t f(\tau) \dd{\tau}} = \frac{F(s)}{s}}
\item \textbf{Convoluzione (nel tempo)} \rarr \boxed{\lt{f_1(t) \circledast f_2(t)} = \lt{\int_0^t f_1(t-\tau)f_2(\tau) \dd{\tau}} =  F_1(s)F_2(s)} : \\ \\ il \textbf{prodotto di convoluzione} è definito come l'integrale del prodotto tra una funzione ferma e una seconda che trasla sulla prima. NB: supponiamo che le funzioni $f_1,f_2$ siano nulle per $t < 0$. Nel dominio di Laplace, questo si trasforma in un prodotto semplice tra le trasformate.
\end{itemize}

\section{Trasformate di segnali elementari}

\resource{0.6}{lap_elem}{Segnali elementari (1)}
\resource{0.6}{lap_elem2}{Segnali elementari (2)}
\bb
\textbf{Osservazione:} è importante focalizzarci sul gradino di Heaviside $1(t)$, perché è quella funzione che, moltiplicata per una qualsiasi altra $f(t)$, la rende \textbf{causale}, i.e. nulla per $t < 0$ e inalterata per $t \geq 0$. Poiché, come già detto, vogliamo avere a che fare solo con robe di questo tipo, qualunque funzione potremmo vederla come il prodotto della stessa con il gradino unitario. Un esempio immediato è la \textbf{funzione rampa} $t1(t)$ di cui si riporta sopra la trasformata: nulla fino a $0$ da sinistra, poi lineare $y=t$. 

\chapter{Sistemi LTI nel dominio di Laplace}
Prima di analizzare i sistemi LTI in esclusiva, riportiamo questo risultato che si applica in generale a tutti i sistemi \textbf{lineari}.
\begin{prop}
Abbiamo visto che, per un sistema lineare descritto nel dominio dei tempi, è possibile scrivere traiettoria di stato ed uscita come una \textbf{somma di un'evoluzione libera ed una forzata} (vedi \eqref{sec:traj_somma_evoluz}). Anche nel dominio di Laplace questo è possibile. In particolare, vale:
\begin{equation}
X(s) = X_L(s) + X_F(s), \quad \quad Y(s) = Y_L(s) + Y_F(s)
\end{equation}
\end{prop}

\section{Equazioni delle trasformate delle traiettorie}
Se consideriamo un sistema LTI, è possibile \textbf{calcolare le equazioni esplicite della trasformata della traiettoria di stato e di uscita}, evidenziando le due evoluzioni che la compongono, analogamente a quanto fatto in \eqref{subsec:eq_traj_lti_scalare_gen}. Analizziamo direttamente il caso generale (i.e.  $x \in \R^n, y \in \R^p, u \in \R^m$): 

%\begin{defin}{}{}
\begin{equation*}
\begin{dcases}
\dot x(t) = Ax(t) + Bu(t) \\
y(t) = Cx(t) + Du(t)
\end{dcases}, \quad x(0) = x_0
\end{equation*}
Definiamo ora le \textbf{trasformate di Laplace} di stato, ingresso e uscita:
\begin{equation}
X(s) \coloneqq \lt{x(t)}, \quad \quad Y(s) \coloneqq \lt{y(t)}, \quad \quad U(s) \coloneqq \lt{u(t)}
\end{equation}
Trasformiamo entrambi i membri delle equazioni, sfruttando le proprietà di linearità e derivazione:
\begin{equation*}
\begin{dcases}
\lt{\dot x(t)} = sX(s) - x(0) = AX(s) + BU(s)\\
Y(s) = CX(s) + DU(s) 
\end{dcases} \rightarrow 
\begin{dcases}
sX(s) - AX(s) = x(0) + BU(s) \\
Y(s) = CX(s) + DU(s) 
\end{dcases} = (\star)
\end{equation*}
Raccogliamo al primo membro e moltiplichiamo $s$ per la matrice identica ($n \times n$) $I$, in modo da rendere matematicamente possibile una differenza di matrici\footnote{
Moltiplicare uno scalare $k$ per una matrice $M$ equivale a moltiplicare $k$ per l'identica $I$, e poi moltiplicare per $M$.}. Esplicitiamo poi $X(s)$ dalla prima equazione:

\begin{equation*}
(\star) \rightarrow
\begin{dcases}
(sI-A)X(s) = x_0 + BU(s) \\
Y(s) = CX(s) + DU(s) 
\end{dcases} \rightarrow \begin{dcases}
X(s) = (sI-A)^{-1}x_0 + (sI-A)^{-1}BU(s) \\
Y(s) = CX(s) + DU(s) 
\end{dcases}
\end{equation*}
Sostituendo l'espressione della $X(s)$ all'interno dell'equazione di uscita, si ha
\begin{center}
\boxed{
\begin{tabular}{ccccccc}
    $X(s)$ & $=$ & \parbox{2cm}{\centering
    $(sI-A)^{-1}$} &
  $x_0$ & $+$ & \parbox{3cm}{\centering
    $(sI-A)^{-1}B$} & $U(s)$ \\[0.13cm]
    $Y(s)$ & $=$ & \parbox{2cm}{\centering
    $C(sI-A)^{-1}$} &
  $x_0$ & $+$ & \parbox{3cm}{\centering
    $\sparen{C(sI-A)^{-1}B + D}$} & $U(s)$ \\
\end{tabular}
}
\end{center}
\subsubsection{Trasformate dell'evoluzione libera e forzata per stato ed uscita}
\begin{align}
X_L(s) = (sI-A)^{-1}x_0, \quad & \quad Y_L(s) = C(sI-A)^{-1}x_0 \\
\label{eq:forz}
X_F(s) = (sI-A)^{-1} BU(s), \quad & \quad Y_F(s) = \sparen{C(sI-A)^{-1}B + D}U(s)
\end{align}
%\end{defin}

Vediamo prima qualche risultato che ci tornerà utile in seguito.
\section{Nozioni di calcolo matriciale}
\begin{prop}
Presa una generica matrice a valori complessi $M \in \M{m}{n}{\C}$, definiamo la \textbf{matrice aggiunta} come:
\begin{equation}
M^\dag = \paren{\hat M}^T\end{equation}
i.e. la trasposta dei \textbf{complementi algebrici} di $M$. Il complemento algebrico (o \textit{cofattore}) di un generico elemento $a_{ij} \in M$ è definito come:
\begin{equation}
\hat{M}_{ij} = \cof{a_{ij}} = (-1)^{i+j} \det(M_{ij})
\end{equation}
ossia il determinante di $M$ alla quale è soppressa la $i$-esima riga e la $j$-esima colonna, preso positivo se $i+j$ è pari, dispari altrimenti. 

\end{prop}
\begin{prop}
Il \textbf{metodo di Laplace} per il calcolo del determinante di una \textbf{generica matrice $M$ quadrata} si basa sui complementi algebrici. In particolare, \textbf{fissata una riga $i$ (o una colonna $j$) qualsiasi di $M$}, il determinante è pari a:
\begin{equation} \underbrace{\sum_{j=1}^n a_{ij} \cof{a_{ij}}}_{\textrm{riga $i$ fissata}} = \det(A) = \underbrace{\sum_{i=1}^n a_{ij} \cof{a_{ij}}}_{\textrm{colonna $j$ fissata}}
\end{equation}
Si sommano cioè i prodotti fra gli \textbf{elementi della riga (o della colonna) scelta} - quindi si itera sulla colonna se fisso una riga, su una riga se fisso una colonna - e \textbf{i rispettivi complementi algebrici}. Il metodo ha un approccio ricorsivo fin quando non si arriva ad una $3\times 3$, per la quale si può usare Sarrus, o $2\times 2$, per la quale invece esiste una formula specifica.
\end{prop}
\begin{prop}
Usando la definizione di matrice aggiunta, possiamo calcolare l'\textbf{inversa} di una generica matrice \textbf{quadrata} $M \in \M{n}{n}{\C}$ usando la seguente relazione:
\begin{equation}
A^{-1} = \frac{A^\dag}{\det(A)}
\end{equation}
\end{prop}

\begin{esem}
Consideriamo $A$ e calcoliamo $(sI-A)^{-1}$. Abbiamo bisogno del suo determinante e della sua matrice aggiunta.
\begin{equation*}
A = \begin{bmatrix}
0 & 1 \\ \alpha & \beta 
\end{bmatrix} \quad\longrightarrow \quad sI-A = \begin{bmatrix}
s & -1 \\ -\alpha & s- \beta
\end{bmatrix}
\end{equation*}

\subsubsection{Determinante di $sI-A$}
\begin{equation*}
\det(sI-A) = s(s-\beta) -\alpha = s^2 - \beta s -\alpha
\end{equation*}
\subsubsection{Matrice aggiunta di $sI-A$}
Calcoliamo la matrice di cofattori (solo il primo viene scritto esplicitamente) e trasponiamola (chiamo $sI-A = K$ per brevità in questi passaggi):
\begin{align*}
\cof{K_{11}} = (-1)^{1+1} \det\paren{\begin{bmatrix}
s-\beta
\end{bmatrix}} = s-\beta \quad & \quad
\cof{K_{12}} = -(-\alpha) = \alpha \\
\cof{K_{21}} = -(-1) = 1 \quad & \quad \cof{K_{22}} = s
\end{align*}
da cui:
\begin{equation*}
(sI-A)^\dag = \paren{\hat{sI-A}}^T = \begin{bmatrix}
s-\beta & \alpha \\ 1 & s
\end{bmatrix}^T = \begin{bmatrix}
s-\beta & 1 \\
\alpha & s
\end{bmatrix}
\end{equation*}
\subsubsection{Calcolo della matrice inversa}
\begin{equation*}
(sI-A)^{-1} = \frac{(sI-A)^\dag}{\det(sI-A)} = \begin{bmatrix}
\displaystyle\frac{s-\beta}{s^2-\beta s -\alpha} & \displaystyle\frac{1}{s^2-\beta s -\alpha} \\[0.4cm] 
\displaystyle\frac{\alpha}{s^2-\beta s -\alpha} & \displaystyle\frac{s}{s^2-\beta s -\alpha}
\end{bmatrix}
\end{equation*}
\end{esem}
\newpage
\section{Funzione di trasferimento}
\begin{defin}{}{}
Recuperiamo la definizione dell'\textbf{evoluzione forzata dell'uscita} scritta in \eqref{eq:forz}. Chiamiamo  \textbf{funzione di trasferimento} la matrice:
\begin{equation}
\label{eq:_trasf}
G(s) = C(sI-A)^{-1}B + D
\end{equation}
Nel caso in cui il sistema sia SISO, avremmo $B$ $(n\times 1)$, $C$ $(1 \times n)$, $D$ $(1 \times 1)$, per cui $G(s)$ è scalare. La nuova rappresentazione ingresso-uscita (forzata) diventa quindi, nel caso generale:
\begin{equation}
\label{eq:gs_evoforz_y}
Y_F(s) = G(s)U(s)
\end{equation}
Se invece supponiamo di avere un sistema senza evoluzione libera, quindi con stato iniziale \textit{nullo}, l'equazione di sopra diventa l'espressione totale della (trasformata della) traiettoria di uscita. Da questa si ricava la relazione tra la funzione di trasferimento e le trasformate di uscita/ingresso:
\begin{equation}
	x(0) = x_0 = 0 \quad \rightarrow \quad Y(s) = G(s)U(s) \quad \textrm{da cui} \quad \boxed{G(s) = \frac{Y(s)}{U(s)}}
\end{equation}
Conoscere $G(s)$ è quindi potentissimo, in quanto mi definisce da sola il comportamento del sistema in analisi, permettendomi immediatamente di calcolare l'uscita corrispondente ad un ingresso.
\bb
Facendo uso dei richiami matriciali di inizio sezione, definiamo operativamente la funzione di trasferimento, a partire dalla \eqref{eq:_trasf}:
\begin{equation}
G(s) = C \frac{(sI-A)^\dag}{\det(sI-A)}B+D
\end{equation} 
\end{defin}
Notiamo che il determinante è un polinomio, dunque quel rapporto non è altro che una moltiplicazione di una matrice $n\times n$ per uno scalare. Il risultato dell'inversa è dunque ancora una $n\times n$.
\begin{itemize}
\item caso generale \rarr $C$ è una $p \times n$, e $B$ è una $n \times m$, dunque il prodotto ordinato tra le tre dà in uscita una $p \times m$, che corrisponde alle dimensioni di $D$. $G$ è dunque una \textbf{matrice} i cui elementi sono rapporti di polinomi, giusto?
\item caso SISO \rarr $C$ è una $1\times n$, e $B$ è una $n\times 1$, dunque il prodotto ordinato dà uno scalare, che ancora corrisponde alle dimensioni di $D$. Vediamo meglio:
\end{itemize}

\subsection{$G(s)$ razionale fratta e fattorizzazioni}
Dunque, nel caso SISO, $G$ è scalare, ed in particolare, è una \textbf{funzione razionale fratta} di questo tipo:
\begin{equation}
\label{eq:trasf_polinomi}
\textrm{se SISO} \quad \rightarrow \quad G(s) = \frac{N(s)}{D(s)} = \frac{\beta_m s^m + \beta_{m -1} s^{m -1} + \cdots + \beta_1 s + \beta_0}{s^n + \alpha_{n - 1}s^{n -1} + \cdots + \alpha_1 s + \alpha_0} 
\end{equation}
Vediamo alcune proprietà:
\begin{itemize}
\item chiamiamo \textbf{grado relativo} $r$ la differenza tra il grado di $D$ e quello di $N$ \rarr \boxed{r = n-m} ;
\item essendo $N,D$ polinomi \textbf{a coefficienti reali}, poli e zeri saranno \textbf{o reali o complessi coniugati}. Inoltre, \textbf{i poli sono gli autovalori di $A$}, essendo $D$   composto dalla quantità scalare $\det(sI-A)$, che rappresenta esattamente il \textit{polinomio caratteristico} di $A$;
\item vale, in genere, \boxed{\deg(N) \leq \deg(D)} , cioè $m \leq n$. Ma:
\begin{equation*}
\textrm{se} \ D \neq 0 \quad \rightarrow \quad \deg(N) = \deg(D) = \nu
\end{equation*}
per cui la \eqref{eq:trasf_polinomi} avrà $\nu$ sia al numeratore, che al denominatore. 
\end{itemize}

\newpage
\resource{0.4}{poli_zeri_gauss}{Rappresentazione di poli e zeri sul piano di Gauss.}
\bb
Approfondiamo ora meglio le due forme fattorizzate della \eqref{eq:trasf_polinomi}.

\resource{0.7}{g_s_forme_fatt}{Forme fattorizzate della funzione di trasferimento $G(s)$ per sistemi SISO.}
\bb
Le due produttorie per numeratore e denominatore nella \textbf{prima forma} vogliono evidenziare il fatto che possono esserci poli/zeri reali (i.e. $(s+z_i)$ o $(s+p_i)$) e anche complessi coniugati, scrivibili in forma di polinomio mettendo in gioco gli \textit{smorzamenti} $\zeta, \xi$ e le \textit{pulsazioni naturali} $\alpha, \omega$. Vedremo invece che la \textbf{seconda forma} tornerà utile per lo studio \textit{in frequenza} dei sistemi.
\bb
Da questa rappresentazione è evidente come sia possibile che tra i due polinomi  \textbf{possano esserci semplificazioni}. Questo si traduce in una riduzione del numero di autovalori, ergo  \textbf{perdita di informazioni sul sistema}, cosa che non avviene nel dominio dei tempi. 
%\begin{lemma}
%Uno stato di un sistema si dice raggiungibile se è possibile, mediante opportuna scelta dell'ingresso, condurre verso di esso la traiettoria del sistema in analisi in un tempo finito arbitrario. Un sistema LTI è completamente raggiungibile se e solo se il rango della \textbf{matrice di raggiungibilità}
%\begin{equation*}
%M = \begin{bmatrix}
%B & AB & A^2B & \cdots & A^{n-1}B 
%\end{bmatrix}\in \R^{n\times mn}
%\end{equation*} 
%è pari ad $n$. Nel caso in cui un sistema non sia completamente raggiungibile, è possibile isolare la parte che lo è. Da come abbiamo dato la definzione, la raggiungibilità dipende unicamente dalle matrici $A,B$, dunque dall'equazione di stato.
%\end{lemma}
\begin{esem}
(Semplificazione). Consideriamo il sistema dinamico seguente:
\begin{equation*}
\begin{dcases}
\dot x_1 = -x_1 + x_2  \\
\dot x_2 = -2x_2 + u \\
y=x_2
\end{dcases} \quad \longrightarrow \quad 
\begin{dcases}
\dot x = \begin{bmatrix}
-1 & 1 \\ 0 & -2
\end{bmatrix} x + \begin{bmatrix}
0 \\ 1
\end{bmatrix} u \\
y = \begin{bmatrix}
0 & 1
\end{bmatrix} x + \sparen{0} u
\end{dcases}
\end{equation*}
Passiamo nel dominio di Laplace considerando la funzione di trasferimento (molti passaggi rimossi):
\begin{equation*}
G(s) = C(sI-A)^{-1}B+D = \begin{bmatrix}
0 & 1
\end{bmatrix}\begin{bmatrix}
\frac{s+2}{(s+2)(s+1)} & \frac{1}{(s+2)(s+1)} \\ 0 & \frac{s+1}{(s+2)(s+1)}
\end{bmatrix}\begin{bmatrix}
0 \\ 1
\end{bmatrix} = \frac{\cancel{s+1}}{(s+2)\cancel{(s+1)}} = \frac{1}{s+2} 
\end{equation*}
\end{esem}

\newpage
\section{Antitrasformazione}
\resource{0.5}{schema}{Schema riassuntivo di quanto visto finora per sistemi LTI}
\bb
Facendo riferimento alla figura in alto ricapitoliamo quanto abbiamo visto in queste pagine: partendo dalla rappresentazione in forma di stato nel dominio dei tempi, siamo riusciti, trasformando, a generare un \textbf{problema immagine} nel dominio di Laplace, composto da equazioni che coinvolgono direttamente la trasformata di stato (non la sua derivata) e di uscita. Mediante la funzione di trasferimento $G(s)$, siamo riusciti poi a dare una relazione esplicita tra ingresso ed evoluzione forzata dell'uscita \eqref{eq:gs_evoforz_y}. 
\begin{defin}{}{}
Recuperiamo ora l'equazione di uscita generale per sistemi LTI, e sostituiamo alla parte forzata proprio l'espressione con la funzione $G(s)$:
\begin{equation*}
Y(s) = C(sI-A)^{-1} x_0 + G(s)U(s)
\end{equation*}
Si può far vedere che $C(sI-A)^{-1}$ è una matrice $1 \times n$ i cui elementi sono \textit{rapporti di polinomi}. Anche la $G(s)$ abbiamo visto che, nel caso generale, si presenta come una matrice di rapporti di polinomi (giusto?). Se, nel corso della trattazione,  considereremo \textbf{solamente ingressi $u(t)$ avneti trasformata $U(s)$ scrivibile come un rapporto di polinomi}  (funzioni gradino, sinusoidali, rampa...), allora l'intera $Y(s)$ sarà scrivibile come un rapporto di polinomi: 
\begin{align}
\label{eq:y_rapp_poli}
\textrm{se anche $U(s)$ rapp. polinomi} \quad \rightarrow \quad Y(s) = \frac{N(s)}{D(s)}
\end{align}
Ma a cosa ci servono tutte queste considerazioni? Perché vogliamo che accada questo? Preso un problema nei tempi, una volta trovata una sua soluzione immagine all'interno del dominio di Laplace, \textbf{è necessario tornare indietro per riottenere la sua rispettiva nei tempi}! La formula canonica di antitrasformazione non è molto comoda da utilizzare per farlo se le funzioni in gioco sono complesse, oppure siamo in casi non riconducibili a quelli elementari. Ci viene dunque in aiuto lo \textbf{sviluppo di Heaviside, o in fratti semplici}.

\end{defin}

\begin{prop}
Consideriamo l'equazione dell'uscita nel caso di evoluzione forzata (quindi stato iniziale nullo) vista in \eqref{eq:gs_evoforz_y}. Se consideriamo come ingresso del sistema un impulso in $t=0$, i.e. una delta di Dirac, vale che, essendo la sua trasformata pari ad 1, la trasformata dell'uscita è data direttamente da $G(s)$:
\begin{equation*}
u(t) = \delta(t) \quad \rightarrow \quad Y(s) = G(s) \quad \textrm{ma per la \eqref{eq:y_rapp_poli} ho} \quad \frac{N(s)}{D(s)} = G(s)
\end{equation*} 
questo significa che \textbf{per la risposta all'impulso} le radici del denominatore $D(s)$ corrispondono proprio ai \textbf{poli della funzione di trasferimento $G(s)$.}
\end{prop}

\subsection{Sviluppo di Heaviside o in fratti semplici}
\begin{defin}{}{}
Per poter applicare questo metodo, è necessario che la trasformata dell'uscita $Y(s)$ sia \textbf{scrivibile come un rapporto di polinomi}. L'obiettivo dello sviluppo è ottenere una \textbf{fattorizzazione} di $Y(s)$ per ottenere una semplificazione  nel procedimento di antitrasformazione. 

\subsubsection{Caso 1: poli reali o complessi coniugati semplici (molteplicità 1)}
A partire dalla \eqref{eq:y_rapp_poli}, fattorizziamo il denominatore e scomponiamo quanto ottenuto in una somma di rapporti di polinomi aventi dei \textit{coefficienti} $k_i$, detti \textbf{residui}, al numeratore, e l'i-esimo termine della produttoria al denominatore:
\begin{equation}
Y(s) = \frac{N(s)}{D(s)} = \frac{N(s)}{\prod_{i=1}^n (s+p_i)} = \sum_{i=1}^n \frac{k_i}{s+p_i}
\end{equation}
I residui sono reali, se associati a poli reali, complessi coniugati altrimenti; si calcolano così (la derivazione dell'equazione viene omessa, la si può trovare nella slide 24/38 su Laplace del Prof.):
\begin{equation}
k_i = \eval{(s+p_i)Y(s)}_{s=-p_i} = \eval{(s+p_i)\frac{N(s)}{D(s)}}_{s=-p_i}
\end{equation}
\begin{equation*}
\end{equation*} 
Con Heaviside l'antitrasformazione di $Y(s)$ diventa nettamente più semplice. Grazie alla proprietà di linearità possiamo scrivere:
\begin{equation*}
y(t) = \lat{Y(s)} = \lat{\sum_{i=1}^n \frac{k_i}{s+p_i}} = \sum_{i=1}^n k_i \lat{\frac{1}{s+p_i}}
\end{equation*} 
ma la funzione da antitrasformare è la trasformata della funzione elementare $e^{\alpha t}1(t)$! Dunque:
\begin{equation}
y(t) = \sum_{i=1}^n k_i e^{p_i t}1(t)
\end{equation}
\end{defin}
\begin{esem} (Antitrasformazione con sviluppo di Heaviside per poli reali semplici.) Calcolare l'uscita $y(t)$ di un sistema avente funzione di trasferimento $G(s) = \frac{1}{s+p}$, a fronte dell'ingresso $u(t) = 5(t)$ (gradino di ampiezza $5$). Vale la relazione:
\begin{equation*}
Y(s) = G(s) U(s) =  \frac{\lt{5(t)}}{s+p} = \frac{5\lt{1(t)}}{s+p} = \frac{5}{s(s+p)} = \frac{k_1}{s} + \frac{k_2}{s+p} = (\star) 
\end{equation*}
Calcoliamo i residui:
\begin{equation*}
k_1 = \eval{sY(s)}_{s=0} = \eval{\frac{5}{s+p}}_{s=0} = \frac{5}{p} \quad \quad \quad k_2 = \eval{(s+p)\frac{5}{s(s+p)}}_{s=-p} = \eval{\frac{5}{s}}_{s=-p} = -\frac{5}{p}
\end{equation*}
Sostituiamo:
\begin{equation*}
(\star) = Y(s) = \frac{5}{sp} - \frac{5}{p(s+p)} = \frac{5}{p}\paren{\frac{1}{s} - \frac{1}{s+p}}
\end{equation*}
A questo punto possiamo antitrasformare facilmente, tenendo conto che la quantità $5/p$ è un coefficiente:
\begin{equation*}
y(t) = \frac{5}{p}\paren{\lat{\frac{1}{s}} - \lat{\frac{1}{s+p}}} = \frac{5}{p}1(t)-\frac{5}{p}1(t)e^{pt}.
\end{equation*}
\end{esem}
\newpage
\begin{esem}
Consideriamo la seguente funzione di trasferimento, e calcoliamo l'antitrasformatadell'uscita a fronte dell'ingresso $u(t) = 1(t)$.
\begin{equation*}
G(s) = \frac{1}{s^2+6s+109} \quad \rightarrow \quad Y(s) = G(s) U(s) = \frac{1}{s(s^2+6s+109)} = (\star)
\end{equation*}
Prima di procedere, vediamo che la $G(s)$ è stavolta scritta mettendo in evidenza dei \textit{polinomi} al posto delle radici complesse coniugate. In particolare, dunque, facendo riferimento alla prima forma fattorizzata vista qualche pagina fa, è possibile \textbf{evidenziare gli smorzamenti e le pulsazioni naturali} dei poli:
\begin{equation*}
D(s) = s^2 + \underbrace{6}_{2\xi \omega_n}s + \underbrace{109}_{\omega_n^2} \quad \textrm{da cui} \quad \omega_n = \sqrt{109}, \ \xi = \frac{3}{\sqrt{109}}
\end{equation*}
Fatto questo piccolo excursus, procediamo con lo sviluppo di Heaviside. Il polinomio $s^2 + 6s + 109$ ha due radici complesse coniugate:
\begin{equation*}
s_1=-3+10j \quad \quad s_2=-3-10j
\end{equation*}
per cui possiamo fattorizzarlo, e scrivere diversamente la trasformata dell'uscita\footnote{nell'esercizio cerco di scrivere le radici, quando queste si presentano nella forma $(s-radice)$, come $(s+radice)$, in modo da essere coerente con le formule viste per lo sviluppo di Heaviside, che usano il $+$}:
\begin{align*}
(\star) = Y(s) & = \frac{1}{s\sparen{s-(-3+10j)}\sparen{(s-(-3-10j)}} = \frac{1}{s(s+(3-10j))(s+(3+10j))} \\ & =\frac{k_1}{s} + \frac{k_{2,1}}{s+(3-10j)} +\frac{k_{2,2}}{s+(3+10j)} = (\star_2)
\end{align*}
Calcoliamo i residui:
\begin{equation*}
k_1 = \eval{sY(s)}_{s=0} = \eval{\frac{1}{(s+(3-10j))(s+(3+10j))}}_{s=0} = \frac{1}{(3-10j)(3+10j)} = \frac{1}{109}
\end{equation*}
\begin{align*}
\eval{k_{2,1} = (s+(3-10j))Y(s)}_{s=-(3-10j)} = \eval{\frac{1}{s(s+(3+10j))}}_{s=-3+10j} = \frac{1}{(-3+10j)(20j)} = \frac{1}{-200-60j}
\end{align*}
\begin{align*}
k_{2,2} & = \eval{(s+(3+10j))Y(s)}_{s=-(3+10j)} = \eval{\frac{1}{s(s+(3-10j))}}_{s=-3-10j} = \frac{1}{(-3-10j)(-20j)} = \frac{1}{-200+60j}
\end{align*}
Notiamo come i due residui associati alle radici complesse coniugate siano a loro volta coimplessi coniugati, mentre quello associato ad $s$ sia reale. A questo punto riscriviamo $Y(s)$:
\begin{equation*}
(\star_2) = Y(s) = \frac{\frac{1}{109}}{s} + \frac{\frac{1}{-200-60j}}{s+(3-10j)} +\frac{\frac{1}{-200+60j}}{s+(3+10j)}
\end{equation*}
Sfruttiamo poi la linearità e altre proprietà per calcolare l'antitrasformata (passaggi non mostrati).
\end{esem}











