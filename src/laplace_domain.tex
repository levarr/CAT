% !TEX root = ./main.tex

\part{Analisi nel dominio di Laplace}

\chapter{Laplace $\mathcal{L}\sparen{\cdot}$}
Come già visto in altri ambiti con Steinmetz e Fourier, anche in questo corso può rivelarsi utile spostare un problema dal dominio temporale ad un altro, risolverlo lì, e riportare la soluzione di nuovo nel tempo.
\resource{0.5}{laplace}{Risoluzione nel dominio di Laplace}
I concetti che vedremo faranno forte uso dei numeri complessi, che però \textbf{non} verranno ripresi qui perché sono sempre le solite cose (rappresentazione cartesiana, polare, esponenziale, come passare dall'una all'altra, proprietà varie). Rimando alla dispensa di TLC-1 dove queste cose sono trattate di più.
\section{Trasformata}
\begin{defin}{}{}
Sia $f: \R \rightarrow \C$ (in questo corso tipicamente $f: \R \rightarrow \R$); sia $s = \sigma + j\omega \in \C$. Definiamo la \textbf{trasformata di Laplace} (se esiste) come:
\begin{equation}
	\lt{f(t)} = F(s) = \ltint{f(t)}{-st}{t}
\end{equation} 
Ci muoviamo, quindi, dal dominio del tempo, al \textbf{dominio di Laplace}: $f(t) \xrightarrow{\Lap} F(s)$. Notiamo gli estremi di integrazione: Laplace non è definita, come Fourier, su tutto l'asse reale, ma solo sul \textbf{semiasse positivo} $[0^-, \pinf]$. Lo $0^-$ è incluso perché bisogna tenere conto, in fase di trasformazione, di eventuali \textbf{impulsi in $t = 0$ della $f$}. La funzione integranda è complesso, per cui l'integrale stesso (e dunque $F(s)$) è complesso. \bb Dato il dominio di definizione, si può dire che l'operazione di trasformazione \textbf{non è biunivoca}, i.e. non è vero che ad ogni $f$ corrisponde una e una sola $F$. Potrei prendere delle funzioni del tutto diverse tra loro nell'intervallo $\R^-$, ma poi identiche in $\R^+$: queste avranno la stessa trasformata. Per ottenere biunivocità, considereremo  sempre \textbf{funzioni definite nel semiasse positivo di $\R$, e nulle altrove} (dette \textbf{funzioni causali)}:
\begin{equation}
	f(t) = 0 \quad \forall t < 0^-
\end{equation}
Supponendo di lavorare con sistemi \textit{tempo invarianti}, questa restrizione non comporta perdità di generalità: posso traslare una qualsiasi funzione TI di un certo valore temporale $t_0$ per riportarla completamente nel dominio $[0^-, \pinf]$, e si comporterà identicamente a come si comporterebbe se non fosse traslata. 
\end{defin}

\begin{prop}
Si può far vedere che:
\begin{equation}
	\exists \bar \sigma \ : \ \forall s = \sigma + j \omega \ \textrm{avente} \ \sigma > \bar \sigma \ \longrightarrow \ F(s) = \int \cdots \dd{t} \ \textrm{converge}.
\end{equation}
Esiste cioè un'ascissa $\bar \sigma$ sul piano complesso, detta \textbf{di convergenza}, a destra della quale si trovano tutti i complessi che fanno \textbf{convergere l'integrale} della trasformazione di Laplace. Dunque, \textbf{la trasformata esiste a destra di quest'ascissa} (i.e. nel semipiano $\Re{s} > \bar \sigma$), (ma sarà possibile estendere l'esistenza anche a sinistra di $\bar \sigma$, i.e. per $\Re{s} \leq \bar \sigma$).
\end{prop}

\subsection{Trasformata come rapporto di polinomi}
Vedremo nel corso $f(t)$ aventi sempre come trasformata un \textbf{rapporto di polinomi} (a coefficienti reali, se $f$ è reale):
\begin{equation}
\label{lt_rap}
	F(s) = \frac{N(s)}{D(s)}
\end{equation} 
Le radici del numeratore ($N(s) = 0$) sono dette \textbf{zeri}, quelle del denominatore, \textbf{poli}. Valgono i seguenti:
\begin{them}
(del valore iniziale). Se $f(t)$ è una funzione reale avente trasformata $F(s)$ esprimibile come in \eqref{lt_rap}, e \textbf{il grado di $D(s)$ è maggiore di quello di $N(s)$}, allora
\begin{equation}
\llimit{s}{\pinf}{sF(s)} = f(0).
\end{equation}
\end{them}

\begin{them} (del valore finale). Se alle ipotesi del teorema di cui sopra aggiungiamo anche che \textbf{il denominatore ha tutti i poli nulli o a parte reale negativa}, allora vale
\begin{equation}
\llimit{s}{0}{sF(s)} = \llimit{t}{\pinf}{f(t)}
\end{equation} 
\end{them}
Grazie a questi possiamo determinare i \textbf{valori asintotici }iniziali e finali di una funzione nel tempo \textbf{partendo dalla sua trasformata} di Laplace. Le dimostrazioni sono omesse.

\section{Antitrasformata}
\begin{defin}{}{}
Per tornare nel dominio del tempo ($F(s) \xrightarrow{\Lap^{-1}} f(t)$) usiamo la formula di \textbf{antitrasformazione}:
\begin{equation}
	f(t) = \lat{F(s)} = \latint{F(s)}{st}{s}, \quad \quad \sigma > \bar \sigma.
\end{equation}
Notiamo la restrizione per $\sigma$: abbiamo detto che $F(s)$ esiste a destra di $\bar \sigma$, quindi l'antitrasformazione (che integra $F(s)$ stesso) dovrà rispettare questo requisito. Questa formula non verrà mai utilizzata praticamente.
\end{defin}


\section{Proprietà}
\begin{itemize}
	\item \textbf{Linearità} \rarr \boxed{\lt{\alpha f(t) + \beta g(t)} = \alpha \lt{f(t)} + \beta \lt{g(t)}, \ \forall \alpha, \beta \in \R} \\ \\ si dimostra sfruttando la linearità dell'operatore integrale;
	\item \textbf{Traslazione temporale} \rarr \boxed{\lt{f(t-\tau)} = \lt{f(t)}e^{-s\tau}, \ \forall \tau > 0} : \\ \\ la trasformata di una funzione traslata sarà la trasformata della funzione non traslata, più un termine moltiplicativo che dipende dal quantitativo di traslazione.
	\begin{proof}
	Usiamo la definizione e consideriamo un cambio di variabile $y = t-\tau$, da cui $\dd{y} = \dd{t}$:
	\begin{align*}
		\lt{f(t-\tau)} = F(s) & = \ltint{f(t-\tau)}{-st}{t} = \int_{-\tau}^{\pinf} f(y)e^{-s(y+\tau)} \dd{y} = (\star)
	\end{align*}
	ma abbiamo detto di voler considerare funzioni nulle nel semiasse negativo, per cui possiamo cambiare l'estremo di integrazione:
	\begin{equation*}
		(\star) = \int_{0^-}^{\pinf} f(y)e^{-sy}\dd{y} e^{-s\tau} = \lt{f(t)}e^{-s\tau}.
	\end{equation*}
	\end{proof}
	\item \textbf{Traslazione nel dominio complesso} \rarr \boxed{\lt{e^{\alpha t}f(t)} = F(s-\alpha), \ \forall \alpha \in \C} : \\ \\ la traslazione nel dominio di Laplace  di un certo complesso $\alpha$ corrisponde ad un termine moltiplicativo nel dominio del tempo che dipende dalla translazione;
	\begin{proof}
		\begin{equation*}
		\lt{e^{\alpha t}f(t)}  = \ltint{e^{\alpha t}f(t)}{-st}{t} = \ltint{f(t)}{-(s-\alpha)t}{t} = F(s-\alpha) 		
		\end{equation*}
	\end{proof}
	\item \textbf{Derivazione (nel tempo)} \rarr \boxed{\lt{\dv{f(t)}{t}} = sF(s)-f(0^-)}
		\begin{proof}
	Usiamo la definizione di trasformata e applichiamo l'integrazione per parti:
	\begin{align*}
		\lt{\dv{f(t)}{t}} & = \ltint{\dv{f(t)}{t}}{-st}{t}	 = \eval{f(t)e^{-st}}_{0^-}^{\pinf} - \int_{0^-}^{\pinf} f(t)(-s)e^{-st} \dd{t} \\ & = 0 - f(0^-) + \int_{0^-}^{\pinf} sf(t)e^{-st} \dd{t} = - f(0^-) + s \int_{0^-}^{\pinf} f(t)e^{-st} \dd{t} \\ & = - f(0^-) + s \lt{f(t)} = sF(s) - f(0^-).
	\end{align*}
	\end{proof}
	\item \textbf{Derivazione (nel tempo) generica} \rarr \boxed{\lt{\dv[n]{f(t)}{t}} = s^{n} F(s) - \sum_{i=1}^{n} s^{n-i} \eval{\dv[i-1]{f(t)}{t}}_{t=0^-}} \footnote{
	Verifichiamo che è valida per la derivata prima:
	\begin{equation*}
		\lt{\dv{f(t)}{t}} = s^1 F(s) - \sum_{i=1}^1 \cdots = s F(s) - s^{0} \eval{f(t)}_{0^-} = sF(s)-f(0^-).
	\end{equation*}}
	\item \textbf{Integrazione (nel tempo)} \rarr \boxed{\lt{\int_0^t f(\tau) \dd{\tau}} = \frac{F(s)}{s}}
\item \textbf{Convoluzione (nel tempo)} \rarr \boxed{\lt{f_1(t) \circledast f_2(t)} = \lt{\int_0^t f_1(t-\tau)f_2(\tau) \dd{\tau}} =  F_1(s)F_2(s)} : \\ \\ il \textbf{prodotto di convoluzione} è definito come l'integrale del prodotto tra una funzione ferma e una seconda che trasla sulla prima. NB: supponiamo che le funzioni $f_1,f_2$ siano nulle per $t < 0$. Nel dominio di Laplace, questo si trasforma in un prodotto semplice tra le trasformate.
\end{itemize}

\section{Trasformate di segnali elementari}

\resource{0.6}{lap_elem}{Segnali elementari (1)}
\resource{0.6}{lap_elem2}{Segnali elementari (2)}
\bb
\textbf{Osservazione:} è importante focalizzarci sul gradino di Heaviside $1(t)$, perché è quella funzione che, moltiplicata per una qualsiasi altra $f(t)$, la rende \textbf{causale}, i.e. nulla per $t < 0$ e inalterata per $t \geq 0$. Poiché, come già detto, vogliamo avere a che fare solo con robe di questo tipo, qualunque funzione potremmo vederla come il prodotto della stessa con il gradino unitario. Un esempio immediato è la \textbf{funzione rampa} $t1(t)$ di cui si riporta sopra la trasformata: nulla fino a $0$ da sinistra, poi lineare $y=t$. 

\chapter{Sistemi LTI nel dominio di Laplace}
Prima di analizzare i sistemi LTI in esclusiva, riportiamo questo risultato che si applica in generale a tutti i sistemi \textbf{lineari}.
\begin{prop}
Abbiamo visto che, per un sistema lineare descritto nel dominio dei tempi, è possibile scrivere traiettoria di stato ed uscita come una \textbf{somma di un'evoluzione libera ed una forzata} (vedi \eqref{sec:traj_somma_evoluz}). Anche nel dominio di Laplace questo è possibile. In particolare, vale:
\begin{equation}
X(s) = X_L(s) + X_F(s), \quad \quad Y(s) = Y_L(s) + Y_F(s)
\end{equation}
\end{prop}

\section{Equazioni delle trasformate delle traiettorie}
Se consideriamo un sistema LTI, è possibile \textbf{calcolare le equazioni esplicite della trasformata della traiettoria di stato e di uscita}, evidenziando le due evoluzioni che la compongono, analogamente a quanto fatto in \eqref{subsec:eq_traj_lti_scalare_gen}. Analizziamo direttamente il caso generale (i.e.  $x \in \R^n, y \in \R^p, u \in \R^m$): 

%\begin{defin}{}{}
\begin{equation*}
\begin{dcases}
\dot x(t) = Ax(t) + Bu(t) \\
y(t) = Cx(t) + Du(t)
\end{dcases}, \quad x(0) = x_0
\end{equation*}
Definiamo ora le \textbf{trasformate di Laplace} di stato, ingresso e uscita:
\begin{equation}
X(s) \coloneqq \lt{x(t)}, \quad \quad Y(s) \coloneqq \lt{y(t)}, \quad \quad U(s) \coloneqq \lt{u(t)}
\end{equation}
Trasformiamo entrambi i membri delle equazioni, sfruttando le proprietà di linearità e derivazione:
\begin{equation*}
\begin{dcases}
\lt{\dot x(t)} = sX(s) - x(0) = AX(s) + BU(s)\\
Y(s) = CX(s) + DU(s) 
\end{dcases} \rightarrow 
\begin{dcases}
sX(s) - AX(s) = x(0) + BU(s) \\
Y(s) = CX(s) + DU(s) 
\end{dcases} = (\star)
\end{equation*}
Raccogliamo al primo membro e moltiplichiamo $s$ per la matrice identica ($n \times n$) $I$, in modo da rendere matematicamente possibile una differenza di matrici\footnote{
Moltiplicare uno scalare $k$ per una matrice $M$ equivale a moltiplicare $k$ per l'identica $I$, e poi moltiplicare per $M$.}. Esplicitiamo poi $X(s)$ dalla prima equazione:

\begin{equation*}
(\star) \rightarrow
\begin{dcases}
(sI-A)X(s) = x_0 + BU(s) \\
Y(s) = CX(s) + DU(s) 
\end{dcases} \rightarrow \begin{dcases}
X(s) = (sI-A)^{-1}x_0 + (sI-A)^{-1}BU(s) \\
Y(s) = CX(s) + DU(s) 
\end{dcases}
\end{equation*}
Sostituendo l'espressione della $X(s)$ all'interno dell'equazione di uscita, si ha
\begin{center}
\boxed{
\begin{tabular}{ccccccc}
    $X(s)$ & $=$ & \parbox{2cm}{\centering
    $(sI-A)^{-1}$} &
  $x_0$ & $+$ & \parbox{3cm}{\centering
    $(sI-A)^{-1}B$} & $U(s)$ \\[0.13cm]
    $Y(s)$ & $=$ & \parbox{2cm}{\centering
    $C(sI-A)^{-1}$} &
  $x_0$ & $+$ & \parbox{3cm}{\centering
    $\sparen{C(sI-A)^{-1}B + D}$} & $U(s)$ \\
\end{tabular}
}
\end{center}
\subsubsection{Trasformate dell'evoluzione libera e forzata per stato ed uscita}
\begin{align}
X_L(s) = (sI-A)^{-1}x_0, \quad & \quad Y_L(s) = C(sI-A)^{-1}x_0 \\
\label{eq:forz}
X_F(s) = (sI-A)^{-1} BU(s), \quad & \quad Y_F(s) = \sparen{C(sI-A)^{-1}B + D}U(s)
\end{align}
%\end{defin}

\section{Descrizione tramite funzione di trasferimento}
Vediamo prima qualche risultato che ci tornerà utile in seguito.
\subsection{Nozioni di calcolo matriciale}
\begin{prop}
Presa una generica matrice a valori complessi $M \in \M{m}{n}{\C}$, definiamo la \textbf{matrice aggiunta} come:
\begin{equation}
M^\dag = \paren{\hat M}^T\end{equation}
i.e. la trasposta dei \textbf{complementi algebrici} di $M$. Il complemento algebrico (o \textit{cofattore}) di un generico elemento $a_{ij} \in M$ è definito come:
\begin{equation}
\hat{M}_{ij} = \cof{a_{ij}} = (-1)^{i+j} \det(M_{ij})
\end{equation}
ossia il determinante di $M$ alla quale è soppressa la $i$-esima riga e la $j$-esima colonna, preso positivo se $i+j$ è pari, dispari altrimenti. 

\end{prop}
\begin{prop}
Il \textbf{metodo di Laplace} per il calcolo del determinante di una \textbf{generica matrice $M$ quadrata} si basa sui complementi algebrici. In particolare, \textbf{fissata una riga $i$ (o una colonna $j$) qualsiasi di $M$}, il determinante è pari a:
\begin{equation} \underbrace{\sum_{j=1}^n a_{ij} \cof{a_{ij}}}_{\textrm{riga $i$ fissata}} = \det(A) = \underbrace{\sum_{i=1}^n a_{ij} \cof{a_{ij}}}_{\textrm{colonna $j$ fissata}}
\end{equation}
Si sommano cioè i prodotti fra gli \textbf{elementi della riga (o della colonna) scelta} - quindi si itera sulla colonna se fisso una riga, su una riga se fisso una colonna - e \textbf{i rispettivi complementi algebrici}. Il metodo ha un approccio ricorsivo fin quando non si arriva ad una $3\times 3$, per la quale si può usare Sarrus, o $2\times 2$, per la quale invece esiste una formula specifica.
\end{prop}
\begin{prop}
Usando la definizione di matrice aggiunta, possiamo calcolare l'\textbf{inversa} di una generica matrice \textbf{quadrata} $M \in \M{n}{n}{\C}$ usando la seguente relazione:
\begin{equation}
A^{-1} = \frac{A^\dag}{\det(A)}
\end{equation}
\end{prop}

\begin{defin}{}{}
Recuperiamo la definizione dell'\textbf{evoluzione forzata dell'uscita} scritta in \eqref{eq:forz}. Chiamiamo  \textbf{funzione di trasferimento} la matrice:
\begin{equation}
\label{eq:_trasf}
G(s) = C(sI-A)^{-1}B + D
\end{equation}
Nel caso in cui il sistema sia SISO, avremmo $B$ $(n\times 1)$, $C$ $(1 \times n)$, $D$ $(1 \times 1)$, per cui $G(s)$ è scalare. La nuova rappresentazione ingresso-uscita diventa quindi, nel caso generale:
\begin{equation}
Y_F(s) = G(s)U(s)
\end{equation}
Se invece supponiamo di avere un sistema senza evoluzione libera, quindi con stato iniziale \textit{nullo}, l'equazione di sopra diventa l'espressione totale della (trasformata della) traiettoria di uscita. Da questa si ricava la relazione tra la funzione di trasferimento e le trasformate di uscita/ingresso:
\begin{equation}
	x(0) = x_0 = 0 \quad \rightarrow \quad Y(s) = G(s)U(s) \quad \textrm{da cui} \quad \boxed{G(s) = \frac{Y(s)}{U(s)}}
\end{equation}
Conoscere $G(s)$ è quindi potentissimo, in quanto mi definisce da sola il comportamento del sistema in analisi, permettendomi immediatamente di calcolare l'uscita corrispondente ad un ingresso.
\bb
Facendo uso dei richiami matriciali di inizio sezione, definiamo operativamente la funzione di trasferimento, a partire dalla \eqref{eq:_trasf}:
\begin{equation}
G(s) = C \frac{(sI-A)^\dag}{\det(sI-A)}B+D
\end{equation} 
\end{defin}
Notiamo che il determinante è un polinomio, dunque quel rapporto non è altro che una moltiplicazione di una matrice $n\times n$ per uno scalare. Il risultato dell'inversa è dunque ancora una $n\times n$.
\begin{itemize}
\item caso generale \rarr $C$ è una $p \times n$, e $B$ è una $n \times m$, dunque il prodotto ordinato tra le tre dà in uscita una $p \times m$, che corrisponde alle dimensioni di $D$;
\item caso SISO \rarr $C$ è una $1\times n$, e $B$ è una $n\times 1$, dunque il prodotto ordinato dà uno scalare, che ancora corrisponde alle dimensioni di $D$.
\end{itemize}
In quest'ultimo caso, dunque, si ha $G(s)$ \textbf{funzione razionale fratta}, i.e. rapporto di polinomi:
\begin{equation}
\textrm{se SISO} \quad \rightarrow \quad G(s) = \frac{N(s)}{D(s)} = \frac{\beta_\nu s^\nu + \beta_{\nu -1} s^{\nu -1} + \cdots + \beta_1 s + \beta_0}{s^\nu + \alpha_{\nu - 1}s^{\nu -1} + \cdots + \alpha_1 s + \alpha_0} 
\end{equation}
I polinomi in questione sono \textbf{a coefficienti reali}, per cui poli e zeri saranno \textbf{o reali o complessi coniugati}. Inoltre, il denominatore non è altro che il polinomio derivante da $\det(sI-A)$, dunque i poli sono gli \textbf{autovalori di $A$} (non per forza tutti, potrebbero esserci semplificazioni con il numeratore). 

 
\newpage
\begin{esem}
Consideriamo $A$ e calcoliamo $(sI-A)^{-1}$. Abbiamo bisogno del suo determinante e della sua matrice aggiunta.
\begin{equation*}
A = \begin{bmatrix}
0 & 1 \\ \alpha & \beta 
\end{bmatrix} \quad\longrightarrow \quad sI-A = \begin{bmatrix}
s & -1 \\ -\alpha & s- \beta
\end{bmatrix}
\end{equation*}

\subsubsection{Determinante di $sI-A$}
\begin{equation*}
\det(sI-A) = s(s-\beta) -\alpha = s^2 - \beta s -\alpha
\end{equation*}
\subsubsection{Matrice aggiunta di $sI-A$}
Calcoliamo la matrice di cofattori (solo il primo viene scritto esplicitamente) e trasponiamola (chiamo $sI-A = K$ per brevità in questi passaggi):
\begin{align*}
\cof{K_{11}} = (-1)^{1+1} \det\paren{\begin{bmatrix}
s-\beta
\end{bmatrix}} = s-\beta \quad & \quad
\cof{K_{12}} = -(-\alpha) = \alpha \\
\cof{K_{21}} = -(-1) = 1 \quad & \quad \cof{K_{22}} = s
\end{align*}
da cui:
\begin{equation*}
(sI-A)^\dag = \paren{\hat{sI-A}}^T = \begin{bmatrix}
s-\beta & \alpha \\ 1 & s
\end{bmatrix}^T = \begin{bmatrix}
s-\beta & 1 \\
\alpha & s
\end{bmatrix}
\end{equation*}
\subsubsection{Calcolo della matrice inversa}
\begin{equation*}
(sI-A)^{-1} = \frac{(sI-A)^\dag}{\det(sI-A)} = \begin{bmatrix}
\displaystyle\frac{s-\beta}{s^2-\beta s -\alpha} & \displaystyle\frac{1}{s^2-\beta s -\alpha} \\[0.4cm] 
\displaystyle\frac{\alpha}{s^2-\beta s -\alpha} & \displaystyle\frac{s}{s^2-\beta s -\alpha}
\end{bmatrix}
\end{equation*}
\begin{center}
TO BE CONTINUED... (? wtf)
\end{center}
\end{esem}

















